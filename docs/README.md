# Oppen Project - ML/LLMOps Learning Platform

A comprehensive, locally-runnable ML/LLMOps platform built on Kubernetes and designed to evolve from simple training pipelines to production-grade infrastructure.

## ğŸ“š Documentation Index

### Phase Guides (Sequential)

1. **[Phase 1-4: Foundation](./phase1_overview.md)** âœ… Complete
   - Agent framework (LangGraph)
   - Memory system (short/long-term)
   - Kubernetes setup (Kind cluster)
   - Basic monitoring

2. **[Phase 5: Training Pipeline](./phase5_implementation.md)** âœ… Complete
   - MLflow experiment tracking
   - MNIST & LLM training scripts
   - Containerization
   - Cost tracking system
   - Agent tools for training

   **Summary**: [Phase 5 Completion](./phase5_completion_summary.md)
   - What was built
   - Testing results
   - Known issues & fixes
   - Performance baseline

3. **[Phase 6: GPU Scheduling & Optimization](./phase6_planning.md)** ğŸ“‹ Planned
   - GPU resource allocation
   - Cost optimization engine
   - Prometheus + Grafana monitoring
   - Job queue & batch scheduling
   - Advanced agent tools

4. **[Phase 7+: Advanced Features](./phase6_planning.md#optional-phase-7---advanced-topics)** ğŸ”® Future
   - Distributed training
   - Model serving (KServe)
   - AutoML hyperparameter tuning
   - Data pipelines
   - Compliance & auditing

---

## ğŸ¯ Quick Start

### Prerequisites
```bash
# Check you have:
- Docker desktop with WSL2
- kubectl CLI
- Kind cluster (oppen-local)
- Python 3.10+ with uv
```

### Deploy & Test Phase 5
```bash
# 1. Deploy infrastructure
cd terraform && terraform apply -auto-approve && cd ..

# 2. Start MLflow UI
kubectl port-forward -n ml-training svc/mlflow-server 5000:5000 &

# 3. Run training locally
export MLFLOW_TRACKING_URI=http://localhost:5000
python scripts/training/train_mnist.py --epochs 2

# 4. Explore agent tools
uv run -m app.main repl
> list mlflow experiments
> get experiment metrics <run_id>
```

### Access UIs
- **MLflow**: http://localhost:5000 (experiment tracking)
- **Kubernetes**: `kubectl get pods -A` (cluster health)

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agent (LangGraph)                         â”‚
â”‚  - Trigger training jobs                                    â”‚
â”‚  - Monitor progress                                         â”‚
â”‚  - Analyze costs                                            â”‚
â”‚  - Make optimization decisions                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kubernetes (Kind Local Cluster)                 â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   ml-training ns     â”‚      â”‚  kube-system         â”‚    â”‚
â”‚  â”‚                      â”‚      â”‚                      â”‚    â”‚
â”‚  â”‚  - MLflow Server     â”‚      â”‚  - NVIDIA Plugin     â”‚    â”‚
â”‚  â”‚    (SQLite backend)  â”‚      â”‚  - Prometheus        â”‚    â”‚
â”‚  â”‚                      â”‚      â”‚  - Grafana           â”‚    â”‚
â”‚  â”‚  - Training Jobs     â”‚      â”‚                      â”‚    â”‚
â”‚  â”‚    (MNIST/LLM)       â”‚      â”‚                      â”‚    â”‚
â”‚  â”‚                      â”‚      â”‚                      â”‚    â”‚
â”‚  â”‚  - ConfigMaps        â”‚      â”‚                      â”‚    â”‚
â”‚  â”‚    (Cost rates)      â”‚      â”‚                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Training Infrastructure                         â”‚
â”‚                                                              â”‚
â”‚  Docker Images (in Kind node)                               â”‚
â”‚  - oppen-training-mnist:latest (8.2GB)                      â”‚
â”‚  - oppen-training-llm:latest (8.4GB)                        â”‚
â”‚                                                              â”‚
â”‚  Data Storage                                               â”‚
â”‚  - SQLite (MLflow metadata)                                 â”‚
â”‚  - Artifacts (emptyDir for dev)                             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Phase Progression

| Phase | Focus | Status | Effort |
|-------|-------|--------|--------|
| 1-4 | Foundation | âœ… Complete | 100h |
| 5 | Training Pipeline | âœ… Complete | 60h |
| 6 | GPU Scheduling & Optimization | ğŸ“‹ Planned | 120h |
| 7+ | Advanced Features | ğŸ”® Future | 200h+ |

---

## ğŸ› ï¸ Core Components

### Agent Tools (Phase 5)
```python
# Training management
trigger_training_job(model_type, epochs, lr, batch_size, gpu_enabled)
get_training_status(job_name)

# Experiment tracking
list_mlflow_experiments(experiment_name)
get_experiment_metrics(run_id)

# Cost analysis
calculate_training_cost(job_name)
```

**Phase 6 additions:**
```python
# GPU & resources
get_gpu_availability()
validate_job_resources(model, epochs, batch_size)

# Cost optimization
recommend_cost_optimization(run_id)
forecast_training_cost(job_name, remaining_epochs)

# Job management
queue_training_job(model, priority, name)
get_job_queue_status()
```

### Training Scripts
- **MNIST**: Simple CNN on MNIST dataset (5 min runtime)
- **LLM**: DistilGPT2 fine-tuning on WikiText-2 (10 min runtime)
- Both log metrics: loss, accuracy, duration, GPU/CPU costs

### Infrastructure as Code
- **Terraform**: K8s resources (namespace, deployment, service, configmap)
- **Docker**: Training containers with PyTorch + MLflow
- **YAML**: Job manifests generated at runtime

---

## ğŸ“ˆ Learning Path

### For ML Engineers
1. **Phase 5**: Understand training orchestration and MLflow
2. **Phase 6**: Learn GPU scheduling and cost optimization
3. **Phase 7**: Explore distributed training and model serving

### For MLOps/DevOps
1. **Phase 1-4**: Master Kubernetes and agent framework
2. **Phase 5**: Deploy production training infrastructure
3. **Phase 6**: Implement monitoring, quotas, and multi-tenancy
4. **Phase 7**: Build K8s operators and advanced deployments

### For Data Scientists
1. **Phase 5**: Run training jobs and track experiments
2. **Phase 6**: Understand cost implications and optimization
3. **Phase 7**: Use AutoML and advanced hyperparameter tuning

---

## ğŸ” Key Concepts

### MLflow
- **Experiment Tracking**: Record training runs with metrics
- **Parameters**: Document model configs (epochs, lr, batch_size)
- **Metrics**: Track loss, accuracy, duration, costs
- **Artifacts**: Store trained models and plots

### Kubernetes on Local Machine
- **Kind**: Local K8s cluster for development
- **Namespaces**: Isolate workloads (ml-training, kube-system)
- **Jobs**: Run training to completion once
- **Deployments**: Long-running services (MLflow)
- **ConfigMaps**: Store configuration (cost rates)

### Cost Tracking
- **GPU Cost**: $0.25/hour (configurable)
- **CPU Cost**: $0.05/hour per core (configurable)
- **Logged**: Duration + rates = total cost per run
- **Phase 6**: Add optimization and forecasting

### Agent Architecture
- **Tools**: Functions agent can call with parameters
- **Natural Language**: Agent parses user intent
- **Decision Making**: Agent chooses which tool + parameters
- **Feedback Loop**: Agent uses tool output to refine decisions

---

## ğŸš€ Performance Baselines (Phase 5)

| Metric | MNIST | LLM |
|--------|-------|-----|
| Training Time | ~5 min (2 epochs) | ~10 min (1 epoch) |
| Final Accuracy | 98.7% | Perplexity TBD |
| Cost per Run | ~$0.02 | ~$0.04 |
| GPU Memory | ~500MB | ~2GB |
| Container Size | 8.2GB | 8.4GB |

---

## ğŸ”§ Common Tasks

### Run Training Locally
```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
python scripts/training/train_mnist.py --epochs 3 --batch-size 128
```

### Submit Training via Agent
```bash
uv run -m app.main repl
> trigger training job mnist with 2 epochs
```

### Monitor Training
```bash
# Stream logs
kubectl logs -n ml-training <job-pod> -f

# Check status
kubectl get jobs -n ml-training
kubectl describe job <job-name> -n ml-training
```

### View Experiments
```bash
# Via MLflow UI
# http://localhost:5000

# Via API
curl http://localhost:5000/api/2.0/mlflow/experiments/list
```

### Calculate Costs
```bash
uv run -m app.main repl
> calculate training cost mnist-training-20260103-172145
# Output: Duration: 245.3s, CPU: $0.0068, GPU: $0.0137, Total: $0.0205
```

---

## ğŸ“š Documentation Files

```
docs/
â”œâ”€â”€ README.md                          â† You are here
â”œâ”€â”€ phase1_overview.md                 â† Foundation phases (1-4)
â”œâ”€â”€ phase5_implementation.md           â† Training pipeline details
â”œâ”€â”€ phase5_completion_summary.md       â† What was accomplished
â”œâ”€â”€ phase6_planning.md                 â† Next phase roadmap
â”œâ”€â”€ troubleshooting.md                 â† Common issues (if needed)
â””â”€â”€ architecture_diagrams.md           â† Visual references (optional)
```

---

## âš ï¸ Known Limitations

### Phase 5
- Large image loads (8GB) are slow with `kind load`
- SQLite backend fine for dev, not for production
- Model artifact logging has endpoint mismatch (workaround: try-catch)
- No multi-tenancy or access control

### General
- Local Kind cluster has ~5GB available for containers
- Network bandwidth limits large dataset downloads
- No persistent storage (data lost on pod restart)

### Workarounds
- Use local training script for iteration (faster)
- Schedule large downloads during off-peak hours
- For production: use cloud K8s + PostgreSQL + cloud storage

---

## ğŸ“ What You'll Learn

### Technical Skills
- âœ… Kubernetes job orchestration
- âœ… Container orchestration and image management
- âœ… Infrastructure as Code (Terraform)
- âœ… Experiment tracking and MLOps
- âœ… Cost monitoring and optimization
- âœ… GPU resource scheduling (Phase 6)
- âœ… Monitoring and observability (Phase 6)

### MLOps Best Practices
- âœ… Reproducible training pipelines
- âœ… Metrics and experiment tracking
- âœ… Cost-aware decision making
- âœ… Production deployment patterns
- âœ… Multi-tenancy and RBAC
- âœ… Job queuing and prioritization (Phase 6)

### Agent Integration
- âœ… Tool design for agent use
- âœ… Natural language understanding
- âœ… Autonomous decision making
- âœ… Cost-aware agent workflows

---

## ğŸ¤ Contributing / Extending

### Add New Training Script
1. Create `scripts/training/train_<model>.py`
2. Log metrics with `mlflow.log_metric()`
3. Create `Dockerfile.training-<model>`
4. Update `job_manager.py` to support new type

### Add New Agent Tool
1. Create function in `app/core/tools.py` with `@tool` decorator
2. Register in `app/core/agent.py` `get_tools()` function
3. Document with docstring explaining parameters
4. Test in REPL: `uv run -m app.main repl`

### Deploy to Production
- Replace SQLite with PostgreSQL in Terraform
- Replace emptyDir with PersistentVolumeClaim
- Add RBAC and namespace isolation
- Deploy Prometheus + Grafana (Phase 6)
- Configure alerts and auto-scaling

---

## ğŸ“ Support & Troubleshooting

### Common Issues

**MLflow connection refused:**
```bash
# Check MLflow is running
kubectl get pods -n ml-training

# Check port-forward
kubectl port-forward -n ml-training svc/mlflow-server 5000:5000
```

**Training job stuck in Pending:**
```bash
# Check pod events
kubectl describe pod <pod-name> -n ml-training

# Check logs
kubectl logs <pod-name> -n ml-training
```

**Image pull errors:**
```bash
# Reload image to Kind
kind load docker-image <image-name> --name oppen-local
```

**Out of memory:**
```bash
# Check resource limits
kubectl top nodes
kubectl top pods -n ml-training
```

See [Troubleshooting Guide](./phase5_implementation.md#troubleshooting) for more.

---

## ğŸ”„ Development Workflow

```
1. Modify training script (scripts/training/)
   â†“
2. Test locally: python scripts/training/train_*.py
   â†“
3. Verify MLflow logging
   â†“
4. Rebuild Docker image (if needed)
   â†“
5. Load to Kind: kind load docker-image
   â†“
6. Test agent tool: uv run -m app.main repl
   â†“
7. Iterate on Phase 5 or move to Phase 6
```

---

## ğŸ“… Next Steps

**Short term (Phase 5 completion):**
- [ ] Run local training to completion
- [ ] Verify MLflow shows all metrics
- [ ] Test all 5 agent tools in REPL
- [ ] Document any issues found

**Medium term (Phase 6):**
- [ ] Implement GPU scheduling
- [ ] Build cost optimization engine
- [ ] Deploy Prometheus + Grafana
- [ ] Create advanced agent tools

**Long term (Phase 7+):**
- [ ] Distributed training
- [ ] Model serving pipeline
- [ ] AutoML and HPO
- [ ] Production deployment

---

## ğŸ“ Summary

**Oppen** is your journey to becoming an ML/LLMOps expert. It starts simple (Phase 5: training pipeline) and scales to production-grade infrastructure (Phase 6-7). Every phase adds value:

- **Phase 5**: Run training jobs, track experiments, understand costs
- **Phase 6**: Optimize resources, manage jobs efficiently, real-time monitoring
- **Phase 7+**: Distribute work, serve models, automate everything

This foundation is **locally-runnable**, **production-ready**, and **extensible** for real-world ML workloads.

---

**Questions or feedback?** Check the phase-specific documentation or review the implementation files directly.

**Ready to continue?** Start with Phase 6: [GPU Scheduling & Cost Optimization](./phase6_planning.md)
